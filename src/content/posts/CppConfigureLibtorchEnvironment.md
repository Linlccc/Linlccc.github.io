---
date: 2024-06-20Â 10:20:00
description: ""
tags:
  - C++
  - LibTorch
langs:
  - C++
title: C++Â é…ç½®Â libtorchÂ ç¯å¢ƒ
---

## å‰ç½®æ¡ä»¶

### æ˜¾å¡é©±åŠ¨

åœ¨å‘½ä»¤è¡Œè¾“å…¥`nvidia-smi`æŸ¥çœ‹é©±åŠ¨ä¿¡æ¯ï¼Œå¦‚æœä¿¡æ¯å¼‚å¸¸æˆ– CUDA æ”¯æŒç‰ˆæœ¬è¾ƒä½çš„è¯[å‰å¾€ä¸‹è½½é©±åŠ¨](https://www.nvidia.cn/Download/index.aspx)

```plaintext
 +-----------------------------------------------------------------------------------------+
 | NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |
 |-----------------------------------------+------------------------+----------------------+
 | GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
 |                                         |                        |               MIG M. |
 |=========================================+========================+======================|
 |   0  NVIDIA GeForce RTX 1660 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |
 | N/A   63C    P0             23W /   80W |    1117MiB /   6144MiB |      2%      Default |
 |                                         |                        |                  N/A |
 +-----------------------------------------+------------------------+----------------------+

 +-----------------------------------------------------------------------------------------+
 | Processes:                                                                              |
 |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
 |        ID   ID                                                               Usage      |
 |=========================================================================================|
 |    0   N/A  N/A      1848    C+G   ...2\Enterprise\Common7\IDE\devenv.exe      N/A      |
 |    0   N/A  N/A      6928    C+G   ...nt.CBS_cw5n1h2txyewy\SearchHost.exe      N/A      |
 +-----------------------------------------------------------------------------------------+
```

### CUDA Toolkit

1. [å‰å¾€ä¸‹è½½ CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)
2. CUDA çš„ç‰ˆæœ¬éœ€è¦å°äºç­‰äºé©±åŠ¨ä¸­çš„ `CUDA Version`
3. **é‡ç‚¹ï¼ï¼è¿™é‡Œçš„ CUDA ç‰ˆæœ¬éœ€è¦å’Œ LibTorch ä½¿ç”¨çš„ CUDA ç‰ˆæœ¬ä¸€è‡´**ï¼Œå»ºè®®å…ˆç¡®å®šå¥½ LibTorch è¦ä½¿ç”¨çš„ CUDA ç‰ˆæœ¬
4. ä¸‹è½½å®Œæˆåæ— è„‘ä¸‹ä¸€æ­¥å³å¯,é»˜è®¤å®‰è£…ç›®å½•`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\[version]`
5. éªŒè¯å®‰è£…
   1. åœ¨å‘½ä»¤è¡Œè¾“å…¥`nvcc --version`æŸ¥çœ‹ CUDA ç‰ˆæœ¬ä¿¡æ¯
   2. åœ¨å‘½ä»¤è¡Œè¿è¡Œ **[å®‰è£…ç›®å½•]\extras\demo_suite** ç›®å½•ä¸‹çš„ `bandwidthTest.exe`ã€`deviceQuery.exe`ï¼Œç¡®ä¿åœ¨æœ€åéƒ½æœ‰è¾“å‡º `Result = PASS`

### cuDNN

cuDNN(NVIDIA CUDAÂ® Deep Neural Network library) æ˜¯ NVIDIA ä¸“é—¨é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDeep Neural Networksï¼‰ä¸­çš„åŸºç¡€æ“ä½œè€Œè®¾è®¡åŸºäº GPU çš„åŠ é€Ÿåº“

1. [å‰å¾€ä¸‹è½½ cuDNN](https://developer.nvidia.com/cudnn-downloads)
2. ä¸‹è½½å®Œæˆè§£å‹åï¼Œå°†ç›®å½•ä¸­çš„ binã€includeã€lib æ–‡ä»¶å¤¹æ‹·è´åˆ° CUDA Toolkit çš„å®‰è£…ç›®å½•

## ä¸‹è½½ LibTorch

1. [å‰å¾€ä¸‹è½½ LibTorch](https://pytorch.org/)**ï¼ï¼è¿™é‡Œä¸‹è½½çš„ LibTorch ä½¿ç”¨çš„ CUDA ç‰ˆæœ¬éœ€è¦ä¸ä½ çš„ CUDA ç‰ˆæœ¬ä¸€è‡´**
2. ä¸‹è½½å®Œæˆåå°†æ–‡ä»¶è§£å‹åˆ°ä½ æƒ³å­˜æ”¾çš„ç›®å½•ï¼Œå¦‚è§£å‹åˆ°`S:\libtorch`
3. æŒ‰ä¸‹ win é”® > è¾“å…¥"ç¼–è¾‘ç³»ç»Ÿç¯å¢ƒå˜é‡" > "ç¯å¢ƒå˜é‡" > åœ¨ç³»ç»Ÿå˜é‡ä¸­çš„"Path"æ·»åŠ `S:\libtorch\lib`

## vs2022 ä¸­çš„é…ç½®

> CUDA toolkit ç›®å½•ï¼š`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1`
> LibTorch ç›®å½•ï¼š`S:\libtorch\lib`
> ä»¥ä¸‹é…ç½®éƒ½å°†åœ¨ä»¥ä¸Šå®‰è£…ç›®å½•çš„åŸºç¡€ä¸Šè¿›è¡Œ

åˆ›å»ºä¸€ä¸ª C++ æ§åˆ¶å°é¡¹ç›®

### é…ç½®é¡¹ç›®å±æ€§

1. é¡¹ç›®å³é”® > å±æ€§
2. **å¸¸è§„**

   - **C++è¯­è¨€æ ‡å‡†**ï¼šå°†â€œè¯­è¨€æ ‡å‡†â€è®¾ç½®ä¸º`ISO C++ 17 æ ‡å‡† (/std:c++17)`æˆ–æ›´é«˜ç‰ˆæœ¬

3. **C/C++**

   - **å¸¸è§„** > **é™„åŠ åŒ…å«ç›®å½•**ï¼šæ·»åŠ ä»¥ä¸‹ç›®å½•
     - `S:\libtorch\include`
     - `S:\libtorch\include\torch\csrc\api\include`
     - `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\include`

4. **é“¾æ¥å™¨**

   - **å¸¸è§„** > **é™„åŠ åº“ç›®å½•**ï¼šæ·»åŠ ä»¥ä¸‹ç›®å½•

     - `S:\libtorch\lib`
     - `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\lib\x64`

   - **è¾“å…¥** > **é™„åŠ ä¾èµ–é¡¹**ï¼šæ·»åŠ ä»¥ä¸‹åº“

     ```plaintext
     c10.lib
     c10_cuda.lib
     torch.lib
     torch_cpu.lib
     torch_cuda.lib
     cudart.lib
     cublas.lib
     curand.lib
     ```

   - **å‘½ä»¤è¡Œ** > **å…¶ä»–é€‰é¡¹**ï¼šè¾“å…¥ä»¥ä¸‹å†…å®¹(å¦‚æœä¸æ·»åŠ çš„è¯ä¸èƒ½ä½¿ç”¨ CUDA)

     ```plaintext
     /INCLUDE:?warp_size@cuda@at@@YAHXZ
     /INCLUDE:?_torch_cuda_cu_linker_symbol_op_cuda@native@at@@YA?AVTensor@2@AEBV32@@Z

     # å¦‚æœå¼‚å¸¸çš„è¯å°±åªè¦ç¬¬ä¸€è¡Œï¼Œæˆ‘æµ‹è¯•çš„æ—¶å€™æœ‰ç¬¬äºŒè¡Œä¼šæŠ¥é”™ï¼ˆè¿™æ®µç½‘ä¸Šæ‰¾çš„ï¼Œä¸å¤ªæ¸…é™¤å…·ä½“åŸå› ğŸ˜…ï¼‰
     ```

## æµ‹è¯•

1. [ä¸‹è½½æ ·å“](./LibtorchSample.7z)
   1. éœ€è¦å°†é‡Œé¢çš„**é™„åŠ åŒ…å«ç›®å½•**ã€**é™„åŠ åº“ç›®å½•**æ›¿æ¢æˆä½ çš„ç›®å½•
2. æµ‹è¯•ä»£ç 

   ```cpp
   #include <cuda_runtime.h>
   #include <iostream>
   #include <torch/torch.h>
   #include <torch/version.h>

   // å°† SM ç‰ˆæœ¬è½¬æ¢ä¸ºæ ¸å¿ƒæ•°çš„è¾…åŠ©å‡½æ•°
   int _ConvertSMVer2Cores(int major, int minor);

   int main() {
   // ç‰ˆæœ¬ä¿¡æ¯
   std::cout << "LibTorch ç‰ˆæœ¬ï¼š" << TORCH_VERSION << std::endl;

   // æ£€æŸ¥æ˜¯å¦æ”¯æŒCUDA
   if (torch::cuda::is_available()) {
   std::cout << "CUDA å¯ç”¨ï¼ä½¿ç”¨ GPU" << std::endl;

   int device_count = 0;
   cudaGetDeviceCount(&device_count);
   std::cout << "CUDA è®¾å¤‡æ•°é‡ï¼š" << device_count << std::endl;

   for (int i = 0; i < device_count; ++i) {
   cudaDeviceProp prop;
   cudaGetDeviceProperties(&prop, i);
   std::cout << "è®¾å¤‡ " << i << ": " << prop.name << std::endl;
   std::cout << "  æ€»å†…å­˜ï¼š" << prop.totalGlobalMem / (1024 * 1024) << " MB" << std::endl;
   std::cout << "  å¤šå¤„ç†å™¨ï¼š" << prop.multiProcessorCount << std::endl;
   std::cout << "  CUDA æ ¸å¿ƒï¼š" << prop.multiProcessorCount * _ConvertSMVer2Cores(prop.major, prop.minor) << std::endl;
   std::cout << "  CUDA åŠŸèƒ½ï¼š" << prop.major << "." << prop.minor << std::endl;
   }
   } else {
   std::cout << "CUDA ä¸å¯ç”¨ã€‚æ­£åœ¨ä½¿ç”¨ CPUã€‚" << std::endl;
   }

   // åˆ›å»ºå¼ é‡
   torch::Tensor tensor = torch::rand({ 2, 3 });
   std::cout << "éšæœºå¼ é‡ï¼š" << tensor << std::endl;

   // åŸºæœ¬è¿ç®—
   torch::Tensor tensor_a = torch::tensor({ 1, 2, 3 }, torch::kFloat32);
   torch::Tensor tensor_b = torch::tensor({ 4, 5, 6 }, torch::kFloat32);
   torch::Tensor result = tensor_a + tensor_b;
   std::cout << "å¼ é‡æ·»åŠ ï¼š" << result << std::endl;

   // è‡ªåŠ¨æ±‚å¯¼
   torch::Tensor x = torch::tensor({ 1.0, 2.0, 3.0 }, torch::requires_grad());
   torch::Tensor y = x * 2;
   y.backward(torch::ones_like(y));
   std::cout << "å¡åº¦ï¼š" << x.grad() << std::endl;

   // ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹
   struct Model : torch::nn::Module {
   Model() {
   fc = register_module("fc", torch::nn::Linear(3, 1));
   }

   torch::Tensor forward(torch::Tensor x) {
   return fc->forward(x);
   }

   torch::nn::Linear fc{ nullptr };
   };

   auto model = std::make_shared<Model>();
   torch::optim::SGD optimizer(model->parameters(), torch::optim::SGDOptions(0.01));

   // æ¨¡æ‹Ÿä¸€äº›æ•°æ®
   torch::Tensor inputs = torch::rand({ 10, 3 });
   torch::Tensor targets = torch::rand({ 10, 1 });

   // è®­ç»ƒæ¨¡å‹
   for (size_t epoch = 1; epoch <= 100; ++epoch) {
   optimizer.zero_grad();
   torch::Tensor outputs = model->forward(inputs);
   torch::Tensor loss = torch::mse_loss(outputs, targets);
   loss.backward();
   optimizer.step();

   if (epoch % 10 == 0) {
   std::cout << "Epoch [" << epoch << "/100], Loss: " << loss.item<float>() << std::endl;
   }
   }

   // CUDA æµ‹è¯•
   if (torch::cuda::is_available()) {
   torch::Tensor tensor_cuda = torch::rand({ 2, 3 }, torch::device(torch::kCUDA));
   std::cout << "CUDA ä¸Šçš„éšæœºå¼ é‡ï¼š" << tensor_cuda << std::endl;

   torch::Tensor result_cuda = tensor_cuda * 2;
   std::cout << "CUDA ä¸Šçš„å¼ é‡ä¹˜æ³•ï¼š" << result_cuda << std::endl;
   }

   return 0;
   }

   int _ConvertSMVer2Cores(int major, int minor) {
   typedef struct {
   int SM;
   int Cores;
   } sSMtoCores;

   sSMtoCores nGpuArchCoresPerSM[] = {
   {0x30, 192}, // Kepler
   {0x32, 192}, // Kepler
   {0x35, 192}, // Kepler
   {0x37, 192}, // Kepler
   {0x50, 128}, // Maxwell
   {0x52, 128}, // Maxwell
   {0x53, 128}, // Maxwell
   {0x60, 64},  // Pascal
   {0x61, 128}, // Pascal
   {0x62, 128}, // Pascal
   {0x70, 64},  // Volta
   {0x72, 64},  // Volta
   {0x75, 64},  // Turing
   {-1, -1} };

   int index = 0;
   while (nGpuArchCoresPerSM[index].SM != -1) {
   if (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {
   return nGpuArchCoresPerSM[index].Cores;
   }
   index++;
   }
   // å¦‚æœæœªæ‰¾åˆ° SMï¼Œåˆ™é»˜è®¤è¿”å›å€¼
   return -1;
   }

   ```

3. è¾“å‡º

   ```plaintext
   LibTorch ç‰ˆæœ¬ï¼š2.3.1
   CUDA å¯ç”¨ï¼ä½¿ç”¨ GPU
   CUDA è®¾å¤‡æ•°é‡ï¼š1
   è®¾å¤‡ 0: NVIDIA GeForce GTX 1660 Ti
   æ€»å†…å­˜ï¼š6143 MB
   å¤šå¤„ç†å™¨ï¼š24
   CUDA æ ¸å¿ƒï¼š1536
   CUDA åŠŸèƒ½ï¼š7.5
   éšæœºå¼ é‡ï¼š 0.2155  0.1309  0.6969
   0.1763  0.3493  0.3799
   [ CPUFloatType{2,3} ]
   å¼ é‡æ·»åŠ ï¼š 5
   7
   9
   [ CPUFloatType{3} ]
   å¡åº¦ï¼š 2
   2
   2
   [ CPUFloatType{3} ]
   Epoch [10/100], Loss: 0.137609
   Epoch [20/100], Loss: 0.107415
   Epoch [30/100], Loss: 0.0934079
   Epoch [40/100], Loss: 0.0868109
   Epoch [50/100], Loss: 0.0836093
   Epoch [60/100], Loss: 0.0819671
   Epoch [70/100], Loss: 0.0810442
   Epoch [80/100], Loss: 0.0804563
   Epoch [90/100], Loss: 0.0800275
   Epoch [100/100], Loss: 0.0796769
   CUDA ä¸Šçš„éšæœºå¼ é‡ï¼š 0.7906  0.9510  0.5951
   0.6755  0.9932  0.4448
   [ CUDAFloatType{2,3} ]
   CUDA ä¸Šçš„å¼ é‡ä¹˜æ³•ï¼š 1.5813  1.9021  1.1902
   1.3511  1.9864  0.8897
   [ CUDAFloatType{2,3} ]
   ```

## å¼‚å¸¸ç¬”è®°

### æ‰¾ä¸åˆ° mkl_avx2.1.dll / mkl_def.1.dll

> Intel MKL FATAL ERROR: Cannot load mkl_avx2.1.dll or mkl_def.1.dll

å³ä½¿ä¸‹è½½äº†æœ€æ–°ç‰ˆæœ¬çš„[intel oneApi](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html)ä¹Ÿä¸è¡Œï¼Œæ˜¯å› ä¸ºä»–ä¾èµ–ä¸ 2021.4 çš„ç‰ˆæœ¬ã€‚å‚è€ƒ[issue](https://github.com/pytorch/pytorch/issues/124009#issuecomment-2156183422)ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹

1. åœ¨è§£å†³æ–¹æ¡ˆç›®å½•åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¤¹`lib`
2. åœ¨å‘½ä»¤è¡Œè¾“å…¥`pip install mkl==2021.4` å®‰è£… mkl 2021.4 ç‰ˆæœ¬
   1. å¦‚æœéœ€è¦ä¸‹è½½æœ€æ–°ç‰ˆæœ¬å‰å¾€[intel oneApi](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html)ï¼Œæ³¨æ„æœ€æ–°ç‰ˆä¸­ä¸åŒ…å« mkl_avx2.1.dllã€mkl_def.1.dll
3. ä» python åº“å®‰è£…ç›®å½•ä¸­å°†ä¾èµ–æ‹·è´åˆ°**æ­¥éª¤ 1**çš„æ–‡ä»¶å¤¹ä¸­
   1. python åº“é»˜è®¤å®‰è£…ç›®å½•`C:\Users\[User]\AppData\Local\Programs\Python\Python312\Library\bin`
   2. è¿™é‡Œåªéœ€è¦æ‹·è´ mkl_avx2.1.dllã€mkl_def.1.dll å³å¯
4. æ‰“å¼€ Vs > é¡¹ç›®å³é”® > **å±æ€§** > **ç”Ÿæˆäº‹ä»¶** > **ç”Ÿæˆåäº‹ä»¶** > **å‘½ä»¤è¡Œ**ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹

   ```plaintext
   set customLibDir=../lib
   :: æ‹·è´è‡ªå®šä¹‰åº“ä¸­çš„çš„dllï¼Œå¦‚æœä¸å­˜åœ¨
   for %%f in (%customLibDir%\*.dll) do (
       if not exist $(OutDir)\%%~nxf (
           xcopy /Y /I "%%f" "$(OutDir)"
       )
   )
   ```

### æ‰¾ä¸åˆ° nvToolsExt

> Failed to find nvToolsExt

å‡ºç°è¿™ä¸ªæƒ…å†µå¤§æ¦‚ç‡æ˜¯ä½¿ç”¨çš„ 12.x ç‰ˆæœ¬çš„ CUDA Toolkitï¼ŒNvidia åœ¨ 12.x ç‰ˆæœ¬ä¸­ç§»é™¤äº† nvToolsExtï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹

1. å‰å¾€[CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive)ä¸‹è½½ 11.8.0 ç‰ˆæœ¬çš„ CUDA Toolkitï¼Œ**æ¨èä¸‹è½½ç½‘ç»œå®‰è£…åŒ…**
2. ä¸‹è½½å®Œæˆåæ‰“å¼€å®‰è£…åŒ…ï¼Œé€‰æ‹©è‡ªå®šä¹‰å®‰è£…ï¼Œåªé€‰æ‹© **Nsight NVTX** å³å¯,å¦‚ä¸‹
   ![NVIDIA_CUDA11.8_Install](./NVIDIA_CUDA11.8_Install.png)
